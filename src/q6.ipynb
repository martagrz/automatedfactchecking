{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "4TDIITSf2bx1"
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "from glob import glob\n",
    "import os\n",
    "import json\n",
    "import re\n",
    "import numpy as np\n",
    "import pickle\n",
    "import string\n",
    "from tqdm import tqdm \n",
    "from collections import defaultdict\n",
    "from skopt import gp_minimize\n",
    "from tensorflow.keras.models import model_from_json\n",
    "import h5py\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "from keras.utils import plot_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import flatten, Sentence, Document, Claim, get_doc, load_claims, load_docs, CoxedFour, load_test_claims, preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "mdAV_DCj2bx5"
   },
   "outputs": [],
   "source": [
    "train_path = '../data/fever-data/train.jsonl'\n",
    "dev_path = '../data/fever-data/dev.jsonl'\n",
    "test_path = '../data/fever-data/test.jsonl'\n",
    "doc_collections = glob('../data/wiki-pages/*.jsonl')\n",
    "glove_path = '../data/glove/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "HqnFpPEA2byF"
   },
   "outputs": [],
   "source": [
    "#create list of given claim ids to perform tf-idf\n",
    "claim_ids_ten = [75397,150448,214861,156709,129629,33087,6744,226034,40190,76253]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "7k56r8xE2bya"
   },
   "outputs": [],
   "source": [
    "def get_sentence(doc_id, sentence_id, doc_collections, doc_lookup):\n",
    "    try:\n",
    "      doc = get_doc(doc_id, doc_collections, doc_lookup)\n",
    "    except KeyError:\n",
    "      return \"\"\n",
    "    sentences = doc.sentences\n",
    "    return sentences[sentence_id]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "a5c9JPo-2byd"
   },
   "outputs": [],
   "source": [
    "with open('../pickle_jar/doc_lookup.pckl','rb') as f: \n",
    "    doc_lookup = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "FjdNctQqe2Qn"
   },
   "outputs": [],
   "source": [
    "def get_preprocessed_features(claims_path,doc_lookup, doc_collections):\n",
    "    claims = load_claims(claims_path, None)[:3000]\n",
    "\n",
    "    #Create data: {claim: [words], evidence: [words]} is a single data point (i.e. claim)\n",
    "    preprocessed_claims = []\n",
    "    claims_tags = []\n",
    "    labels = []\n",
    "    preprocessed_evidence = []\n",
    "    evidences_tags = []\n",
    "\n",
    "    for claim in tqdm(claims):\n",
    "        #print(claim.id)\n",
    "        label = claim.truthfulness\n",
    "        tokenized_claim = claim.sentence\n",
    "        evidences = claim.evidence\n",
    "        tokenized_evidence = flatten([get_sentence(evidence.source, \n",
    "                                                    evidence.string_id, doc_collections, doc_lookup) for evidence in evidences if evidence.source is not None])\n",
    "        \n",
    "        claim_tag = nltk.pos_tag(tokenized_claim)\n",
    "        claim_tag = [tag[1] for tag in claim_tag]\n",
    "        evidence_tag = nltk.pos_tag(tokenized_evidence)\n",
    "        evidence_tag = [tag[1] for tag in evidence_tag]\n",
    "\n",
    "        claims_tags.append(claim_tag)\n",
    "        evidences_tags.append(evidence_tag)\n",
    "\n",
    "        preprocessed_claims.append(tokenized_claim)\n",
    "        preprocessed_evidence.append(tokenized_evidence)\n",
    "        labels.append(label)\n",
    "\n",
    "\n",
    "    labels = np.where(np.array(labels)==\"true\", 1, 0).reshape((-1,1))\n",
    "    \n",
    "    return preprocessed_claims, preprocessed_evidence, labels, claims_tags, evidences_tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "jl6oqk_ifOPg"
   },
   "outputs": [],
   "source": [
    "def get_vocab(word_lists):\n",
    "  vocab = set()\n",
    "  for word_list in word_lists:\n",
    "    for words in word_list:\n",
    "      for word in words: \n",
    "        vocab.add(word)\n",
    "    \n",
    "    \n",
    "  vocab = list(vocab)    \n",
    "\n",
    "  vocab_lookup = {}\n",
    "  i = 1\n",
    "  for word in vocab:\n",
    "      vocab_lookup[word] = i\n",
    "      i += 1\n",
    "      \n",
    "  return vocab_lookup, vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "DAqKpwxVfyq_"
   },
   "outputs": [],
   "source": [
    "def get_maxlen(feature_list):\n",
    "  maxlen = []\n",
    "  for feature in feature_list: \n",
    "    maxlen.append(max(len(entry) for entry in feature))\n",
    "  maxlen = np.max(maxlen)\n",
    "  return maxlen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "3qcpETFmfq-o"
   },
   "outputs": [],
   "source": [
    "def convert_feature(maxlen, vocab_lookup, feature):\n",
    "  \n",
    "  map_feature = [list(map(lambda x: vocab_lookup[x], entry)) for entry in feature]\n",
    "  \n",
    "  for entry in map_feature: \n",
    "    entry += [0]*(maxlen - len(entry))\n",
    "  \n",
    "  map_feature = np.array(map_feature)\n",
    "  return map_feature "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "2Dr6Pvaa1FdC"
   },
   "outputs": [],
   "source": [
    "load_files = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 53
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1834268,
     "status": "ok",
     "timestamp": 1556357605697,
     "user": {
      "displayName": "Marta Grzeskiewicz",
      "photoUrl": "",
      "userId": "11289597759619198382"
     },
     "user_tz": -60
    },
    "id": "PZ_NBIYez0--",
    "outputId": "dd48f18a-87e0-494c-ebc9-5f07d7d0d5b7"
   },
   "outputs": [],
   "source": [
    "if load_files == True: \n",
    "  \n",
    "  with open('../pickle_jar/preprocessed_claims','rb') as f:\n",
    "    preprocessed_claims = pickle.load(f)\n",
    "    \n",
    "  with open('../pickle_jar/claims_tags','rb') as f:\n",
    "    claims_tags = pickle.load(f)\n",
    "    \n",
    "  with open('../pickle_jar/dev_preprocessed_claims','rb') as f:\n",
    "    dev_preprocessed_claims = pickle.load(f)\n",
    "    \n",
    "  with open('../pickle_jar/dev_claims_tags','rb') as f:\n",
    "    dev_claims_tags = pickle.load(f)\n",
    "    \n",
    "  with open('../pickle_jar/preprocessed_evidence','rb') as f:\n",
    "    preprocessed_evidence = pickle.load(f)\n",
    "    \n",
    "  with open('../pickle_jar/evidences_tags','rb') as f:\n",
    "    evidences_tags = pickle.load(f)\n",
    "    \n",
    "  with open('../pickle_jar/dev_preprocessed_evidence','rb') as f:\n",
    "    dev_preprocessed_evidence = pickle.load(f)\n",
    "    \n",
    "  with open('../pickle_jar/dev_evidences_tags','rb') as f:\n",
    "    dev_evidences_tags = pickle.load(f)\n",
    "    \n",
    "  with open('../pickle_jar/tag_vocab_lookup','rb') as f:\n",
    "    tag_vocab_lookup = pickle.load(f)\n",
    "    \n",
    "  with open('../pickle_jar/tag_vocab','rb') as f:\n",
    "    tag_vocab = pickle.load(f)\n",
    "    \n",
    "  with open('../pickle_jar/vocab','rb') as f:\n",
    "    vocab = pickle.load(f)\n",
    "  \n",
    "  with open('../pickle_jar/vocab_lookup','rb') as f:\n",
    "    vocab_lookup = pickle.load(f)\n",
    "   \n",
    "  with open('../pickle_jar/maxlen_claims','rb') as f:\n",
    "    maxlen_claims = pickle.load(f)\n",
    "  \n",
    "  with open('../pickle_jar/maxlen_evidence','rb') as f:\n",
    "    maxlen_evidence = pickle.load(f)\n",
    "    \n",
    "  with open('../pickle_jar/labels','rb') as f:\n",
    "    labels = pickle.load(f)\n",
    "    \n",
    "  with open('../pickle_jar/dev_labels','rb') as f:\n",
    "    dev_labels = pickle.load(f)\n",
    "    \n",
    "  print('Files loaded.')\n",
    "  \n",
    "else: \n",
    "  preprocessed_claims, preprocessed_evidence, labels, claims_tags, evidences_tags = get_preprocessed_features(train_path,doc_lookup, doc_collections)\n",
    "  dev_preprocessed_claims, dev_preprocessed_evidence, dev_labels, dev_claims_tags, dev_evidences_tags = get_preprocessed_features(dev_path,doc_lookup, doc_collections)\n",
    "  \n",
    "  vocab_lookup, vocab = get_vocab([preprocessed_claims, preprocessed_evidence,dev_preprocessed_claims, dev_preprocessed_evidence])\n",
    "  tag_vocab_lookup, tag_vocab = get_vocab([claims_tags, evidences_tags, dev_claims_tags, dev_evidences_tags])\n",
    "  \n",
    "  maxlen_claims = get_maxlen([preprocessed_claims,dev_preprocessed_claims,claims_tags,dev_claims_tags])\n",
    "  maxlen_evidence = get_maxlen([preprocessed_evidence,dev_preprocessed_evidence,evidences_tags,dev_evidences_tags])\n",
    "  \n",
    "  preprocessed_claims = convert_feature(maxlen_claims, vocab_lookup, preprocessed_claims)\n",
    "  claims_tags = convert_feature(maxlen_claims, tag_vocab_lookup, claims_tags)\n",
    "  dev_preprocessed_claims = convert_feature(maxlen_claims, vocab_lookup, dev_preprocessed_claims)\n",
    "  dev_claims_tags = convert_feature(maxlen_claims, tag_vocab_lookup, dev_claims_tags)\n",
    "\n",
    "  preprocessed_evidence = convert_feature(maxlen_evidence, vocab_lookup, preprocessed_evidence)\n",
    "  evidences_tags = convert_feature(maxlen_evidence, tag_vocab_lookup, evidences_tags)\n",
    "  dev_preprocessed_evidence = convert_feature(maxlen_evidence, vocab_lookup, dev_preprocessed_evidence)\n",
    "  dev_evidences_tags = convert_feature(maxlen_evidence, tag_vocab_lookup, dev_evidences_tags)\n",
    "  \n",
    "  feature_list_str = ['preprocessed_claims', 'claims_tags', \n",
    "                'dev_preprocessed_claims', 'dev_claims_tags', \n",
    "                'preprocessed_evidence', 'evidences_tags', \n",
    "                'dev_preprocessed_evidence', 'dev_evidences_tags',\n",
    "                'tag_vocab_lookup', 'tag_vocab', \n",
    "                'vocab', 'vocab_lookup',\n",
    "                'maxlen_claims', 'maxlen_evidence',\n",
    "                'labels','dev_labels']\n",
    "\n",
    "  feature_list = [preprocessed_claims, claims_tags, \n",
    "                  dev_preprocessed_claims, dev_claims_tags, \n",
    "                  preprocessed_evidence, evidences_tags, \n",
    "                  dev_preprocessed_evidence, dev_evidences_tags,\n",
    "                  tag_vocab_lookup, tag_vocab, \n",
    "                  vocab, vocab_lookup,\n",
    "                  maxlen_claims, maxlen_evidence,\n",
    "                  labels,dev_labels]\n",
    "\n",
    "\n",
    "  for feature_str, feature in zip(feature_list_str, feature_list): \n",
    "      with open('../pickle_jar/{}'.format(feature_str),'wb') as f:\n",
    "          pickle.dump(feature,f)\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "NKF7ilsYIutX"
   },
   "outputs": [],
   "source": [
    "with open('../pickle_jar/dev_preprocessed_evidence','wb') as f:\n",
    "  pickle.dump(dev_preprocessed_evidence,f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ke_hrf7p-4YL"
   },
   "source": [
    "# Import GloVE embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 9274,
     "status": "ok",
     "timestamp": 1556357658622,
     "user": {
      "displayName": "Marta Grzeskiewicz",
      "photoUrl": "",
      "userId": "11289597759619198382"
     },
     "user_tz": -60
    },
    "id": "ThQjlnI74u6y",
    "outputId": "a45730d4-fa2a-4cfe-983f-30e0a1abc3cb"
   },
   "outputs": [],
   "source": [
    "dimensions = 50\n",
    "\n",
    "embeddings_index = {}\n",
    "f = open(os.path.join(glove_path, 'glove.6B.{}d.txt'.format(dimensions)))\n",
    "for line in f:\n",
    "    values = line.split()\n",
    "    word = values[0]\n",
    "    coefs = np.asarray(values[1:], dtype='float32')\n",
    "    embeddings_index[word] = coefs\n",
    "f.close()\n",
    "\n",
    "print('Found %s word vectors.' % len(embeddings_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Go1WUbL-4qNU"
   },
   "outputs": [],
   "source": [
    "embedding_matrix = np.zeros((len(vocab_lookup) + 1, dimensions))\n",
    "for word, i in vocab_lookup.items():\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        # words not found in embedding index will be all-zeros.\n",
    "        embedding_matrix[i] = embedding_vector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "9TnJztDZGw-n"
   },
   "source": [
    "# Text classification with an RNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "bjUqGVBxGw-t"
   },
   "source": [
    "## Create the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "bgs6nnSTGw-t"
   },
   "source": [
    "Build a `tf.keras.Sequential` model. We have two input channels, one for the claims and one for the evidence (relevant sentences). Each input channel has two inputs, the sentences themselves and their POS tags. Each of these inputs first goes through an embedding layer. The POS tags go through a trainable embedding layer, whereas the sentences go through the GLOVE embedding layer. These are then concatenated and put through a BiLSTM (so there are two BiLSTMs running in parallel). Then, the output is concatenated and put through a dense layer before it is put through the final layer for two-way classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "hl9uyMlVzjRG"
   },
   "outputs": [],
   "source": [
    "def construct_model(params, maxlen_claims, maxlen_evidence, dimensions,embedding_matrix):\n",
    "\n",
    "    claims_input = tf.keras.Input(shape=[maxlen_claims], dtype=tf.int32)\n",
    "\n",
    "    claims_output = tf.keras.Sequential([\n",
    "        tf.keras.layers.Embedding(len(vocab)+1, dimensions, weights=[embedding_matrix], input_length=maxlen_evidence, trainable=False),\n",
    "        tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(64)),\n",
    "    ])(claims_input)\n",
    "\n",
    "    evidence_input = tf.keras.Input(shape=[maxlen_evidence], dtype=tf.int32)\n",
    "    evidence_output = tf.keras.Sequential([\n",
    "        tf.keras.layers.Embedding(len(vocab)+1, dimensions, weights=[embedding_matrix], input_length=maxlen_evidence, trainable=False),\n",
    "        tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(64))\n",
    "    ])(evidence_input)\n",
    "\n",
    "    common_output = tf.keras.layers.Concatenate()([claims_output, evidence_output])\n",
    "\n",
    "    final_output = tf.keras.Sequential([\n",
    "        tf.keras.layers.Dense(64, activation=\"relu\"),\n",
    "        tf.keras.layers.Dense(1, activation=\"sigmoid\")\n",
    "    ])(common_output)\n",
    "\n",
    "    model = tf.keras.Model(inputs=[claims_input, evidence_input], outputs=final_output)\n",
    "    \n",
    "    model.compile(loss='binary_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])\n",
    "    \n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "LwfoBkmRYcP3"
   },
   "outputs": [],
   "source": [
    "def run_model(params, \n",
    "              maxlen_claims = maxlen_claims, maxlen_evidence = maxlen_evidence, \n",
    "              dimensions = dimensions, embedding_matrix = embedding_matrix, \n",
    "              return_model = False):\n",
    "    \n",
    "    model = construct_model(params, maxlen_claims, maxlen_evidence, dimensions,embedding_matrix)\n",
    "    \n",
    "    history = model.fit([preprocessed_claims, claims_tags, preprocessed_evidence, evidences_tags], \n",
    "                    labels,\n",
    "                    validation_data=([dev_preprocessed_claims, dev_claims_tags, dev_preprocessed_evidence, evidences_tags], dev_labels),\n",
    "                    epochs=params[6])\n",
    "\n",
    "    if return_model:     \n",
    "        \n",
    "        return model, history.history\n",
    "        \n",
    "    else: \n",
    "      \n",
    "        return history.history['val_loss'][-1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "hFPUl4s5JuvL"
   },
   "outputs": [],
   "source": [
    "params = [\n",
    "    # Dimensions for embedding claims tags \n",
    "    35,\n",
    "    # Hidden layers in claims BiLSTM\n",
    "    31,\n",
    "    # Dimensions for embedding evidence tags\n",
    "    34,\n",
    "    # Hidden layers in evidence BiLSTM\n",
    "    23,\n",
    "    # Dense layer\n",
    "    97, \n",
    "    # Dropout rate\n",
    "    0.34,\n",
    "    # Epochs\n",
    "    7\n",
    "    \n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "EXmBhfvmLkPJ"
   },
   "source": [
    "## Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "WOIFAm6Wzi38"
   },
   "outputs": [],
   "source": [
    "# Set only one of the below to be true\n",
    "load_model = True\n",
    "run_final_model = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "O7GsnFipyrOU"
   },
   "outputs": [],
   "source": [
    "if load_model == True: \n",
    "  model = construct_model(params, maxlen_claims, maxlen_evidence, dimensions,embedding_matrix)\n",
    "  model.load_weights(\"../models/model_q8.h5\")\n",
    "  print('Model loaded.')\n",
    "  \n",
    "elif run_final_model == True: \n",
    "  model, history = run_model(params,\n",
    "                    maxlen_claims = maxlen_claims, maxlen_evidence = maxlen_evidence, \n",
    "                    dimensions = dimensions, embedding_matrix = embedding_matrix,\n",
    "                    return_model = True)\n",
    "  \n",
    "  # serialize weights to HDF5\n",
    "  model.save_weights(\"../models/model_q8.h5\")\n",
    "  print(\"Saved weights to disk\")\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "2_DOuGioHDxr"
   },
   "source": [
    "## Performance plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 317
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1452,
     "status": "ok",
     "timestamp": 1556391084391,
     "user": {
      "displayName": "Marta Grzeskiewicz",
      "photoUrl": "",
      "userId": "11289597759619198382"
     },
     "user_tz": -60
    },
    "id": "-whCZSJgrwHM",
    "outputId": "13db916a-132a-4a03-808d-8b642606f2df"
   },
   "outputs": [],
   "source": [
    "if run_final_model:\n",
    "  plt.figure(figsize=(15, 4))\n",
    "  plt.subplot(1,2,1)\n",
    "\n",
    "  plt.plot(model.history.history['loss'])\n",
    "  plt.plot(model.history.history['val_loss'])\n",
    "  plt.xlabel(\"Epochs\")\n",
    "  plt.ylabel('loss')\n",
    "  plt.legend(['loss', 'val_loss'])\n",
    "\n",
    "  plt.subplot(1, 2, 2)\n",
    "  plt.plot(model.history.history['acc'])\n",
    "  plt.plot(model.history.history['val_acc'])\n",
    "  plt.xlabel(\"Epochs\")\n",
    "  plt.ylabel('acc')\n",
    "  plt.legend(['acc', 'val_acc'])\n",
    "\n",
    "  plt.tight_layout()\n",
    "  plt.savefig('plots_q8.png')\n",
    "  plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "BJjsZeCe7xiK"
   },
   "outputs": [],
   "source": [
    "plot_model(model, to_file='../output_files/model_arch_q8.png', show_shapes=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "14010014_Q8.ipynb",
   "provenance": [
    {
     "file_id": "1uzwujiKf0oE-H2fMYOdsD-MKIK1VB7m8",
     "timestamp": 1556033301270
    }
   ],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
